<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.preprocessing.image_enhancement API documentation</title>
<meta name="description" content="Image Enhancement Module for License Plate Preprocessing …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.preprocessing.image_enhancement</code></h1>
</header>
<section id="section-intro">
<p>Image Enhancement Module for License Plate Preprocessing</p>
<p>This module provides preprocessing functions for cropped license plate images
to improve OCR accuracy. Includes grayscale conversion, resizing, and CLAHE
(Contrast Limited Adaptive Histogram Equalization).</p>
<h2 id="note">Note</h2>
<p>Cropping functionality is provided by src.detection.utils.crop_detections()
and should be reused. This module focuses only on enhancement operations.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.preprocessing.image_enhancement.apply_clahe"><code class="name flex">
<span>def <span class="ident">apply_clahe</span></span>(<span>gray_image: numpy.ndarray,<br>clip_limit: float = 2.0,<br>grid_size: Tuple[int, int] = (8, 8)) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_clahe(
    gray_image: np.ndarray, clip_limit: float = 2.0, grid_size: Tuple[int, int] = (8, 8)
) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to grayscale image.

    CLAHE enhances local contrast by dividing the image into tiles and applying
    histogram equalization to each tile with a contrast limiting threshold.
    This improves visibility of license plate text in poor lighting conditions.

    Args:
        gray_image (np.ndarray): Input grayscale image.
            Shape: (H, W). Must be single-channel uint8.
        clip_limit (float, optional): Contrast limiting threshold.
            Higher values increase contrast. Range: [1.0, 40.0]. Default: 2.0.
        grid_size (Tuple[int, int], optional): Size of grid for histogram equalization.
            Smaller tiles = more local adaptation, larger = more global.
            Default: (8, 8).

    Returns:
        np.ndarray: Enhanced grayscale image with improved local contrast.
            Shape: (H, W), dtype: uint8.

    Raises:
        ValueError: If image is not grayscale (single-channel) or not uint8.
        TypeError: If input is not a numpy array.

    Example:
        &gt;&gt;&gt; import cv2
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; # Create a low-contrast grayscale image
        &gt;&gt;&gt; img = np.random.randint(100, 150, (200, 400), dtype=np.uint8)
        &gt;&gt;&gt; enhanced = apply_clahe(img, clip_limit=3.0, grid_size=(8, 8))
        &gt;&gt;&gt; enhanced.dtype
        dtype(&#39;uint8&#39;)
        &gt;&gt;&gt; # Contrast should be improved
        &gt;&gt;&gt; np.std(enhanced) &gt; np.std(img)
        True

    Note:
        - Input must be grayscale (convert color images first)
        - clip_limit=1.0 is equivalent to standard histogram equalization
        - Typical range: 2.0-4.0 for most images
        - Smaller grid_size (e.g., 4x4) for more aggressive local adaptation
        - Larger grid_size (e.g., 16x16) for smoother global adaptation
    &#34;&#34;&#34;
    if not isinstance(gray_image, np.ndarray):
        raise TypeError(f&#34;Input must be numpy array, got {type(gray_image)}&#34;)

    if len(gray_image.shape) != 2:
        raise ValueError(f&#34;Input must be grayscale (2D array), got shape {gray_image.shape}&#34;)

    if gray_image.dtype != np.uint8:
        raise ValueError(f&#34;Input must be uint8, got {gray_image.dtype}&#34;)

    if gray_image.size == 0:
        raise ValueError(&#34;Input image is empty&#34;)

    logger.debug(f&#34;Applying CLAHE with clip_limit={clip_limit}, grid_size={grid_size}&#34;)

    # Create CLAHE object
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)

    # Apply to image
    enhanced = clahe.apply(gray_image)

    return enhanced</code></pre>
</details>
<div class="desc"><p>Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to grayscale image.</p>
<p>CLAHE enhances local contrast by dividing the image into tiles and applying
histogram equalization to each tile with a contrast limiting threshold.
This improves visibility of license plate text in poor lighting conditions.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gray_image</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Input grayscale image.
Shape: (H, W). Must be single-channel uint8.</dd>
<dt><strong><code>clip_limit</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Contrast limiting threshold.
Higher values increase contrast. Range: [1.0, 40.0]. Default: 2.0.</dd>
<dt><strong><code>grid_size</code></strong> :&ensp;<code>Tuple[int, int]</code>, optional</dt>
<dd>Size of grid for histogram equalization.
Smaller tiles = more local adaptation, larger = more global.
Default: (8, 8).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Enhanced grayscale image with improved local contrast.
Shape: (H, W), dtype: uint8.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If image is not grayscale (single-channel) or not uint8.</dd>
<dt><code>TypeError</code></dt>
<dd>If input is not a numpy array.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import cv2
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; # Create a low-contrast grayscale image
&gt;&gt;&gt; img = np.random.randint(100, 150, (200, 400), dtype=np.uint8)
&gt;&gt;&gt; enhanced = apply_clahe(img, clip_limit=3.0, grid_size=(8, 8))
&gt;&gt;&gt; enhanced.dtype
dtype('uint8')
&gt;&gt;&gt; # Contrast should be improved
&gt;&gt;&gt; np.std(enhanced) &gt; np.std(img)
True
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Input must be grayscale (convert color images first)</li>
<li>clip_limit=1.0 is equivalent to standard histogram equalization</li>
<li>Typical range: 2.0-4.0 for most images</li>
<li>Smaller grid_size (e.g., 4x4) for more aggressive local adaptation</li>
<li>Larger grid_size (e.g., 16x16) for smoother global adaptation</li>
</ul></div>
</dd>
<dt id="src.preprocessing.image_enhancement.preprocess_plate"><code class="name flex">
<span>def <span class="ident">preprocess_plate</span></span>(<span>cropped_image: numpy.ndarray, config: dict) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_plate(cropped_image: np.ndarray, config: dict) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Apply preprocessing pipeline to a cropped license plate image.

    This function applies a series of optional preprocessing steps to enhance
    license plate images for improved OCR accuracy. Steps include:
    1. Grayscale conversion (if image is color)
    2. Resizing to minimum width while maintaining aspect ratio
    3. CLAHE enhancement for improved contrast

    All steps are optional and controlled by configuration flags.

    Args:
        cropped_image (np.ndarray): Input image (cropped plate) in BGR or grayscale.
            Shape: (H, W, 3) for color or (H, W) for grayscale.
        config (dict): Preprocessing configuration with keys:
            - &#39;use_clahe&#39; (bool): Enable CLAHE enhancement
            - &#39;clahe_clip_limit&#39; (float): CLAHE clip limit (default: 2.0)
            - &#39;clahe_tile_grid_size&#39; (list): CLAHE grid size (default: [8, 8])
            - &#39;min_width&#39; (int): Minimum width in pixels (default: 200)
            - &#39;use_gaussian_blur&#39; (bool): Enable Gaussian blur (optional)
            - &#39;gaussian_kernel_size&#39; (list): Blur kernel size (optional)
            - &#39;use_sharpening&#39; (bool): Enable sharpening (optional)
            - &#39;sharpen_strength&#39; (float): Sharpening strength (optional)

    Returns:
        np.ndarray: Preprocessed image, typically grayscale.
            Shape: (H&#39;, W&#39;) where W&#39; &gt;= min_width and aspect ratio is preserved.

    Raises:
        ValueError: If input image is empty or invalid.
        TypeError: If input is not a numpy array.

    Example:
        &gt;&gt;&gt; import cv2
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; # Simulate a small cropped plate
        &gt;&gt;&gt; plate = np.random.randint(0, 255, (50, 150, 3), dtype=np.uint8)
        &gt;&gt;&gt; config = {
        ...     &#39;min_width&#39;: 200,
        ...     &#39;use_clahe&#39;: True,
        ...     &#39;clahe_clip_limit&#39;: 2.0,
        ...     &#39;clahe_tile_grid_size&#39;: [8, 8]
        ... }
        &gt;&gt;&gt; enhanced = preprocess_plate(plate, config)
        &gt;&gt;&gt; enhanced.shape[1] &gt;= 200  # Width should be at least 200
        True
        &gt;&gt;&gt; len(enhanced.shape) == 2  # Should be grayscale
        True

    Note:
        - The function creates a copy of the input to avoid modifying the original
        - Grayscale conversion is applied automatically if CLAHE is enabled
        - Optional features (blur, sharpening) are applied if enabled in config
    &#34;&#34;&#34;
    if not isinstance(cropped_image, np.ndarray):
        raise TypeError(f&#34;Input must be numpy array, got {type(cropped_image)}&#34;)

    if cropped_image.size == 0:
        raise ValueError(&#34;Input image is empty&#34;)

    # Create a copy to avoid modifying original
    image = cropped_image.copy()

    # Step 1: Convert to grayscale if needed (for CLAHE or OCR)
    # Most OCR engines work better with grayscale
    if len(image.shape) == 3:
        logger.debug(&#34;Converting BGR to grayscale&#34;)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Step 2: Resize if below minimum width (maintain aspect ratio)
    height, width = image.shape[:2]
    min_width = config.get(&#34;min_width&#34;, 200)

    if width &lt; min_width:
        logger.debug(f&#34;Resizing from width {width} to {min_width} (aspect ratio maintained)&#34;)
        image = resize_maintaining_aspect(image, min_width)

    # Step 3: Optional Gaussian blur for noise reduction
    if config.get(&#34;use_gaussian_blur&#34;, False):
        kernel_size = tuple(config.get(&#34;gaussian_kernel_size&#34;, [3, 3]))
        logger.debug(f&#34;Applying Gaussian blur with kernel {kernel_size}&#34;)
        image = cv2.GaussianBlur(image, kernel_size, 0)

    # Step 4: CLAHE enhancement (optional)
    if config.get(&#34;use_clahe&#34;, False):
        logger.debug(&#34;Applying CLAHE enhancement&#34;)
        clip_limit = config.get(&#34;clahe_clip_limit&#34;, 2.0)
        grid_size = tuple(config.get(&#34;clahe_tile_grid_size&#34;, [8, 8]))
        image = apply_clahe(image, clip_limit, grid_size)

    # Step 5: Optional sharpening
    if config.get(&#34;use_sharpening&#34;, False):
        strength = config.get(&#34;sharpen_strength&#34;, 1.0)
        logger.debug(f&#34;Applying sharpening with strength {strength}&#34;)
        # Create sharpening kernel
        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]], dtype=np.float32)
        kernel = kernel * strength
        kernel[1, 1] = 9 - (strength - 1) * 8  # Adjust center
        image = cv2.filter2D(image, -1, kernel)

    return image</code></pre>
</details>
<div class="desc"><p>Apply preprocessing pipeline to a cropped license plate image.</p>
<p>This function applies a series of optional preprocessing steps to enhance
license plate images for improved OCR accuracy. Steps include:
1. Grayscale conversion (if image is color)
2. Resizing to minimum width while maintaining aspect ratio
3. CLAHE enhancement for improved contrast</p>
<p>All steps are optional and controlled by configuration flags.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cropped_image</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Input image (cropped plate) in BGR or grayscale.
Shape: (H, W, 3) for color or (H, W) for grayscale.</dd>
<dt><strong><code>config</code></strong> :&ensp;<code>dict</code></dt>
<dd>Preprocessing configuration with keys:
- 'use_clahe' (bool): Enable CLAHE enhancement
- 'clahe_clip_limit' (float): CLAHE clip limit (default: 2.0)
- 'clahe_tile_grid_size' (list): CLAHE grid size (default: [8, 8])
- 'min_width' (int): Minimum width in pixels (default: 200)
- 'use_gaussian_blur' (bool): Enable Gaussian blur (optional)
- 'gaussian_kernel_size' (list): Blur kernel size (optional)
- 'use_sharpening' (bool): Enable sharpening (optional)
- 'sharpen_strength' (float): Sharpening strength (optional)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Preprocessed image, typically grayscale.
Shape: (H', W') where W' &gt;= min_width and aspect ratio is preserved.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If input image is empty or invalid.</dd>
<dt><code>TypeError</code></dt>
<dd>If input is not a numpy array.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import cv2
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; # Simulate a small cropped plate
&gt;&gt;&gt; plate = np.random.randint(0, 255, (50, 150, 3), dtype=np.uint8)
&gt;&gt;&gt; config = {
...     'min_width': 200,
...     'use_clahe': True,
...     'clahe_clip_limit': 2.0,
...     'clahe_tile_grid_size': [8, 8]
... }
&gt;&gt;&gt; enhanced = preprocess_plate(plate, config)
&gt;&gt;&gt; enhanced.shape[1] &gt;= 200  # Width should be at least 200
True
&gt;&gt;&gt; len(enhanced.shape) == 2  # Should be grayscale
True
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>The function creates a copy of the input to avoid modifying the original</li>
<li>Grayscale conversion is applied automatically if CLAHE is enabled</li>
<li>Optional features (blur, sharpening) are applied if enabled in config</li>
</ul></div>
</dd>
<dt id="src.preprocessing.image_enhancement.resize_maintaining_aspect"><code class="name flex">
<span>def <span class="ident">resize_maintaining_aspect</span></span>(<span>image: numpy.ndarray, target_width: int, interpolation: int = 2) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resize_maintaining_aspect(
    image: np.ndarray, target_width: int, interpolation: int = cv2.INTER_CUBIC
) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Resize image to target width while maintaining aspect ratio.

    This function scales an image to a specified width while preserving
    the original aspect ratio. Uses high-quality interpolation by default.

    Args:
        image (np.ndarray): Input image (grayscale or color).
            Shape: (H, W) or (H, W, C).
        target_width (int): Desired width in pixels. Must be positive.
        interpolation (int, optional): OpenCV interpolation method.
            Default: cv2.INTER_CUBIC (high quality).
            Options: INTER_NEAREST, INTER_LINEAR, INTER_CUBIC, INTER_LANCZOS4.

    Returns:
        np.ndarray: Resized image with target width and scaled height.
            Shape: (H&#39;, target_width) or (H&#39;, target_width, C).

    Raises:
        ValueError: If target_width &lt;= 0 or image is empty.
        TypeError: If input is not a numpy array.

    Example:
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; # Create a 100x300 image
        &gt;&gt;&gt; img = np.random.randint(0, 255, (100, 300), dtype=np.uint8)
        &gt;&gt;&gt; resized = resize_maintaining_aspect(img, 600)
        &gt;&gt;&gt; resized.shape
        (200, 600)
        &gt;&gt;&gt; # Aspect ratio preserved: 100/300 == 200/600

    Note:
        - INTER_CUBIC is recommended for upscaling (smoother results)
        - INTER_AREA is recommended for downscaling (better quality)
        - For small images being upscaled significantly, INTER_LANCZOS4 may be better
    &#34;&#34;&#34;
    if not isinstance(image, np.ndarray):
        raise TypeError(f&#34;Input must be numpy array, got {type(image)}&#34;)

    if image.size == 0:
        raise ValueError(&#34;Input image is empty&#34;)

    if target_width &lt;= 0:
        raise ValueError(f&#34;Target width must be positive, got {target_width}&#34;)

    height, width = image.shape[:2]

    # Calculate scaling factor
    scale = target_width / width
    new_height = int(height * scale)
    new_size = (target_width, new_height)

    logger.debug(f&#34;Resizing from ({width}, {height}) to {new_size}, scale={scale:.2f}&#34;)

    # Perform resize
    resized = cv2.resize(image, new_size, interpolation=interpolation)

    return resized</code></pre>
</details>
<div class="desc"><p>Resize image to target width while maintaining aspect ratio.</p>
<p>This function scales an image to a specified width while preserving
the original aspect ratio. Uses high-quality interpolation by default.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Input image (grayscale or color).
Shape: (H, W) or (H, W, C).</dd>
<dt><strong><code>target_width</code></strong> :&ensp;<code>int</code></dt>
<dd>Desired width in pixels. Must be positive.</dd>
<dt><strong><code>interpolation</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>OpenCV interpolation method.
Default: cv2.INTER_CUBIC (high quality).
Options: INTER_NEAREST, INTER_LINEAR, INTER_CUBIC, INTER_LANCZOS4.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.ndarray</code></dt>
<dd>Resized image with target width and scaled height.
Shape: (H', target_width) or (H', target_width, C).</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If target_width &lt;= 0 or image is empty.</dd>
<dt><code>TypeError</code></dt>
<dd>If input is not a numpy array.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; # Create a 100x300 image
&gt;&gt;&gt; img = np.random.randint(0, 255, (100, 300), dtype=np.uint8)
&gt;&gt;&gt; resized = resize_maintaining_aspect(img, 600)
&gt;&gt;&gt; resized.shape
(200, 600)
&gt;&gt;&gt; # Aspect ratio preserved: 100/300 == 200/600
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>INTER_CUBIC is recommended for upscaling (smoother results)</li>
<li>INTER_AREA is recommended for downscaling (better quality)</li>
<li>For small images being upscaled significantly, INTER_LANCZOS4 may be better</li>
</ul></div>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.preprocessing" href="index.html">src.preprocessing</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.preprocessing.image_enhancement.apply_clahe" href="#src.preprocessing.image_enhancement.apply_clahe">apply_clahe</a></code></li>
<li><code><a title="src.preprocessing.image_enhancement.preprocess_plate" href="#src.preprocessing.image_enhancement.preprocess_plate">preprocess_plate</a></code></li>
<li><code><a title="src.preprocessing.image_enhancement.resize_maintaining_aspect" href="#src.preprocessing.image_enhancement.resize_maintaining_aspect">resize_maintaining_aspect</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
