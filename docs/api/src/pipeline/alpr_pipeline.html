<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.pipeline.alpr_pipeline API documentation</title>
<meta name="description" content="End-to-End ALPR Pipeline Module …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.pipeline.alpr_pipeline</code></h1>
</header>
<section id="section-intro">
<p>End-to-End ALPR Pipeline Module</p>
<p>This module provides the main ALPRPipeline class that orchestrates all components:
detection, tracking, preprocessing, and recognition to perform complete ALPR
on video frames.</p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.pipeline.alpr_pipeline.ALPRPipeline"><code class="flex name class">
<span>class <span class="ident">ALPRPipeline</span></span>
<span>(</span><span>config_path: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ALPRPipeline:
    &#34;&#34;&#34;
    End-to-end Automatic License Plate Recognition pipeline.

    This class coordinates all ALPR components to process video frames:
    1. Detection + Tracking: Uses YOLOv8 with ByteTrack for plate detection and tracking
    2. Track Management: Creates and updates PlateTrack objects for each detection
    3. Conditional OCR: Runs OCR only when needed based on track state
    4. Cleanup: Removes lost tracks to prevent memory leaks

    Attributes:
        config (dict): Full pipeline configuration loaded from YAML
        detection_model: Loaded YOLOv8 model for plate detection
        ocr_model: Loaded PaddleOCR model for text recognition
        tracks (Dict[int, PlateTrack]): Active tracks indexed by track ID
        frame_count (int): Number of frames processed

    Example:
        &gt;&gt;&gt; pipeline = ALPRPipeline(&#39;configs/pipeline_config.yaml&#39;)
        &gt;&gt;&gt; import cv2
        &gt;&gt;&gt; frame = cv2.imread(&#39;test_frame.jpg&#39;)
        &gt;&gt;&gt; tracks = pipeline.process_frame(frame)
        &gt;&gt;&gt; for track_id, track in tracks.items():
        ...     if track.text:
        ...         print(f&#34;Track {track_id}: {track.text} ({track.ocr_confidence:.2f})&#34;)
    &#34;&#34;&#34;

    def __init__(self, config_path: str):
        &#34;&#34;&#34;
        Initialize ALPR pipeline with configuration.

        Loads configuration from YAML file and initializes all models.

        Args:
            config_path: Path to pipeline configuration YAML file

        Raises:
            FileNotFoundError: If config file doesn&#39;t exist
            ValueError: If configuration validation fails
            RuntimeError: If model initialization fails

        Note:
            - Configuration is automatically validated before use
            - Model loading may take several seconds on first run
            - GPU will be used if available for both detection and recognition
            - Logs detailed initialization information
        &#34;&#34;&#34;
        logger.info(&#34;=&#34; * 60)
        logger.info(&#34;Initializing ALPR Pipeline&#34;)
        logger.info(&#34;=&#34; * 60)

        # Load configuration
        logger.info(f&#34;Loading configuration from: {config_path}&#34;)
        try:
            with open(config_path, &#34;r&#34;, encoding=&#34;utf-8&#34;) as f:
                self.config = yaml.safe_load(f)
            logger.info(&#34;✓ Configuration loaded successfully&#34;)
        except FileNotFoundError:
            logger.error(f&#34;Configuration file not found: {config_path}&#34;)
            raise
        except Exception as e:
            logger.error(f&#34;Failed to load configuration: {e}&#34;)
            raise RuntimeError(f&#34;Configuration loading failed: {e}&#34;)

        # Validate configuration
        logger.info(&#34;Validating configuration...&#34;)
        try:
            # Import validation utility
            from scripts.utils.validate_config import ConfigValidator

            validator = ConfigValidator(config_path)
            is_valid, errors = validator.validate()

            if not is_valid:
                error_msg = &#34;Configuration validation failed:\n&#34; + &#34;\n&#34;.join(
                    f&#34;  - {error}&#34; for error in errors
                )
                logger.error(error_msg)
                raise ValueError(error_msg)

            logger.info(&#34;✓ Configuration validated successfully&#34;)
        except ImportError:
            logger.warning(
                &#34;Configuration validation utility not found, skipping validation&#34;
            )
        except Exception as e:
            logger.error(f&#34;Configuration validation failed: {e}&#34;)
            raise

        # Initialize detection model
        logger.info(&#34;Loading detection model...&#34;)
        try:
            self.detection_model = load_detection_model(
                model_path=self.config[&#34;detection&#34;][&#34;model_path&#34;],
                device=self.config[&#34;detection&#34;][&#34;device&#34;],
            )
            logger.info(&#34;✓ Detection model loaded&#34;)
        except Exception as e:
            logger.error(f&#34;Failed to load detection model: {e}&#34;)
            raise RuntimeError(f&#34;Detection model initialization failed: {e}&#34;)

        # Initialize OCR model
        logger.info(&#34;Loading OCR model...&#34;)
        try:
            self.ocr_model = load_ocr_model(self.config[&#34;recognition&#34;])
            logger.info(&#34;✓ OCR model loaded&#34;)
        except Exception as e:
            logger.error(f&#34;Failed to load OCR model: {e}&#34;)
            raise RuntimeError(f&#34;OCR model initialization failed: {e}&#34;)

        # Initialize track storage
        self.tracks: Dict[int, PlateTrack] = {}
        self.frame_count = 0

        logger.info(&#34;=&#34; * 60)
        logger.info(&#34;ALPR Pipeline Initialization Complete&#34;)
        logger.info(&#34;=&#34; * 60)

    def process_frame(self, frame: np.ndarray) -&gt; Dict[int, PlateTrack]:
        &#34;&#34;&#34;
        Process a single video frame through the complete ALPR pipeline.

        Executes the 4-stage pipeline:
        1. Detection + Tracking: Run YOLOv8 with ByteTrack
        2. Track Management: Update track states
        3. Conditional OCR: Run OCR only when needed
        4. Cleanup: Remove lost tracks

        Args:
            frame: Input frame in BGR format (OpenCV convention)
                  Shape: (height, width, 3)

        Returns:
            Dict[int, PlateTrack]: Dictionary of active tracks indexed by track ID.
                                  Each PlateTrack contains current state including
                                  bbox, text, confidence, and age.

        Raises:
            ValueError: If frame is invalid
            RuntimeError: If processing fails

        Example:
            &gt;&gt;&gt; tracks = pipeline.process_frame(frame)
            &gt;&gt;&gt; print(f&#34;Active tracks: {len(tracks)}&#34;)
            &gt;&gt;&gt; for track_id, track in tracks.items():
            ...     print(f&#34;  {track_id}: {track.text or &#39;No text&#39;} &#34;
            ...           f&#34;(age={track.age}, conf={track.ocr_confidence:.2f})&#34;)

        Note:
            - Track IDs persist across frames (ByteTrack maintains identity)
            - OCR is only run when should_run_ocr() returns True
            - Lost tracks are kept for max_age frames before removal
        &#34;&#34;&#34;
        # Validate input
        if not isinstance(frame, np.ndarray):
            raise ValueError(f&#34;Frame must be numpy array, got {type(frame)}&#34;)
        if frame.ndim != 3 or frame.shape[2] != 3:
            raise ValueError(f&#34;Frame must be BGR image with shape (H, W, 3), got {frame.shape}&#34;)

        logger.debug(f&#34;Processing frame {self.frame_count} (shape={frame.shape})&#34;)

        # ===== Stage 1: Detection + Tracking =====
        logger.debug(&#34;Stage 1: Running detection + tracking&#34;)

        try:
            # Run YOLOv8 tracking (ByteTrack integration)
            results = self.detection_model.track(
                source=frame,
                conf=self.config[&#34;detection&#34;][&#34;confidence_threshold&#34;],
                iou=self.config[&#34;detection&#34;][&#34;iou_threshold&#34;],
                tracker=&#34;bytetrack.yaml&#34;,
                persist=True,  # Crucial: maintains track IDs across frames
                verbose=False,
            )
        except Exception as e:
            logger.error(f&#34;Detection + tracking failed: {e}&#34;)
            raise RuntimeError(f&#34;Detection stage failed: {e}&#34;)

        # ===== Stage 2: Update Track States =====
        logger.debug(&#34;Stage 2: Updating track states&#34;)

        current_track_ids = set()
        ocr_count = 0

        # Parse detection results
        for result in results:
            # Check if any detections exist
            if not hasattr(result, &#34;boxes&#34;) or result.boxes is None:
                logger.debug(&#34;No detections in this frame&#34;)
                continue

            boxes = result.boxes

            # Check if tracks exist
            if boxes.id is None:
                logger.debug(&#34;No tracks assigned by ByteTrack&#34;)
                continue

            # Process each tracked detection
            for box in boxes:
                # Extract track information
                track_id = int(box.id.item())
                bbox = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]
                confidence = float(box.conf.item())

                # Convert bbox to tuple of integers
                bbox_tuple = tuple(map(int, bbox))

                # Create new track or update existing
                if track_id not in self.tracks:
                    self.tracks[track_id] = PlateTrack(
                        track_id=track_id, bbox=bbox_tuple, confidence=confidence
                    )
                    logger.info(
                        f&#34;New track detected: ID={track_id}, bbox={bbox_tuple}, conf={
                            confidence:.3f}&#34;
                    )
                else:
                    self.tracks[track_id].update(bbox=bbox_tuple, confidence=confidence)
                    logger.debug(f&#34;Track updated: ID={track_id}, age={self.tracks[track_id].age}&#34;)

                current_track_ids.add(track_id)

                # ===== Stage 3: Conditional OCR =====
                track = self.tracks[track_id]

                # Check if OCR should be run for this track
                if track.should_run_ocr(self.config[&#34;tracking&#34;]):
                    logger.debug(
                        f&#34;OCR triggered for track {track_id} &#34;
                        f&#34;(age={
                            track.age}, frames_since_last_ocr={
                            track.frames_since_last_ocr})&#34;
                    )

                    try:
                        # Crop plate from frame
                        x1, y1, x2, y2 = bbox_tuple
                        cropped = frame[y1:y2, x1:x2]

                        if cropped.size == 0:
                            logger.warning(f&#34;Empty crop for track {track_id}, skipping OCR&#34;)
                            continue

                        # Use adaptive preprocessing (handled internally by recognize_text)
                        preprocessing_config = self.config.get(&#34;preprocessing&#34;, {})
                        use_adaptive_preprocessing = preprocessing_config.get(
                            &#34;enable_enhancement&#34;, False
                        )

                        # Run OCR with adaptive preprocessing enabled
                        text, ocr_conf = recognize_text(
                            preprocessed_image=cropped,
                            ocr_model=self.ocr_model,
                            config=self.config[&#34;recognition&#34;],
                            enable_adaptive_preprocessing=use_adaptive_preprocessing,
                            preprocessing_config=preprocessing_config,
                        )

                        # Update track with OCR results
                        track.update_text(text=text, confidence=ocr_conf)
                        ocr_count += 1

                        if text:
                            logger.info(
                                f&#39;Track {track_id} recognized: &#34;{text}&#34; (confidence={
                                    ocr_conf:.3f})&#39;
                            )
                        else:
                            logger.debug(f&#34;Track {track_id}: No valid text recognized&#34;)

                    except Exception as e:
                        logger.error(f&#34;OCR failed for track {track_id}: {e}&#34;)
                        # Continue processing other tracks
                        continue

        # ===== Stage 4: Cleanup Lost Tracks =====
        logger.debug(&#34;Stage 4: Cleaning up lost tracks&#34;)

        # Mark tracks that disappeared as lost
        lost_track_ids = set(self.tracks.keys()) - current_track_ids

        for track_id in lost_track_ids:
            self.tracks[track_id].mark_lost()
            logger.debug(f&#34;Track {track_id} marked as lost&#34;)

        # Remove inactive tracks older than max_age
        max_age = self.config[&#34;tracking&#34;].get(&#34;max_age&#34;, 30)
        initial_count = len(self.tracks)

        self.tracks = {
            tid: track
            for tid, track in self.tracks.items()
            if track.is_active or track.age &lt; max_age
        }

        removed_count = initial_count - len(self.tracks)
        if removed_count &gt; 0:
            logger.info(f&#34;Removed {removed_count} old lost tracks&#34;)

        # Increment frame counter
        self.frame_count += 1

        # Log summary
        summary = get_track_summary(self.tracks)
        logger.info(
            f&#39;Frame {self.frame_count}: {summary[&#34;active&#34;]} active tracks, &#39;
            f&#39;{summary[&#34;recognized&#34;]} recognized, {ocr_count} OCR runs&#39;
        )

        return self.tracks

    def reset(self) -&gt; None:
        &#34;&#34;&#34;
        Reset pipeline state (clear all tracks and counters).

        Useful when starting to process a new video or after processing is complete.

        Example:
            &gt;&gt;&gt; pipeline.reset()
            &gt;&gt;&gt; print(f&#34;Tracks cleared: {len(pipeline.tracks)}&#34;)
            Tracks cleared: 0
        &#34;&#34;&#34;
        logger.info(&#34;Resetting pipeline state&#34;)
        self.tracks.clear()
        self.frame_count = 0
        logger.info(&#34;Pipeline reset complete&#34;)

    def get_statistics(self) -&gt; dict:
        &#34;&#34;&#34;
        Get current pipeline statistics.

        Returns:
            dict: Statistics including:
                - frame_count: Total frames processed
                - track_count: Current active track count
                - recognized_count: Tracks with recognized text
                - avg_track_age: Average track age
                - avg_ocr_confidence: Average OCR confidence

        Example:
            &gt;&gt;&gt; stats = pipeline.get_statistics()
            &gt;&gt;&gt; print(f&#34;Processed {stats[&#39;frame_count&#39;]} frames, &#34;
            ...       f&#34;{stats[&#39;recognized_count&#39;]} plates recognized&#34;)
        &#34;&#34;&#34;
        summary = get_track_summary(self.tracks)

        return {
            &#34;frame_count&#34;: self.frame_count,
            &#34;track_count&#34;: summary[&#34;total&#34;],
            &#34;active_count&#34;: summary[&#34;active&#34;],
            &#34;recognized_count&#34;: summary[&#34;recognized&#34;],
            &#34;avg_track_age&#34;: summary[&#34;avg_age&#34;],
            &#34;avg_ocr_confidence&#34;: summary[&#34;avg_ocr_confidence&#34;],
        }

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;String representation for debugging.&#34;&#34;&#34;
        stats = self.get_statistics()
        return (
            f&#34;ALPRPipeline(frames={stats[&#39;frame_count&#39;]}, &#34;
            f&#34;tracks={stats[&#39;track_count&#39;]}, &#34;
            f&#34;recognized={stats[&#39;recognized_count&#39;]})&#34;
        )</code></pre>
</details>
<div class="desc"><p>End-to-end Automatic License Plate Recognition pipeline.</p>
<p>This class coordinates all ALPR components to process video frames:
1. Detection + Tracking: Uses YOLOv8 with ByteTrack for plate detection and tracking
2. Track Management: Creates and updates PlateTrack objects for each detection
3. Conditional OCR: Runs OCR only when needed based on track state
4. Cleanup: Removes lost tracks to prevent memory leaks</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>dict</code></dt>
<dd>Full pipeline configuration loaded from YAML</dd>
<dt><strong><code>detection_model</code></strong></dt>
<dd>Loaded YOLOv8 model for plate detection</dd>
<dt><strong><code>ocr_model</code></strong></dt>
<dd>Loaded PaddleOCR model for text recognition</dd>
<dt><strong><code>tracks</code></strong> :&ensp;<code>Dict[int, PlateTrack]</code></dt>
<dd>Active tracks indexed by track ID</dd>
<dt><strong><code>frame_count</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of frames processed</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; pipeline = ALPRPipeline('configs/pipeline_config.yaml')
&gt;&gt;&gt; import cv2
&gt;&gt;&gt; frame = cv2.imread('test_frame.jpg')
&gt;&gt;&gt; tracks = pipeline.process_frame(frame)
&gt;&gt;&gt; for track_id, track in tracks.items():
...     if track.text:
...         print(f&quot;Track {track_id}: {track.text} ({track.ocr_confidence:.2f})&quot;)
</code></pre>
<p>Initialize ALPR pipeline with configuration.</p>
<p>Loads configuration from YAML file and initializes all models.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config_path</code></strong></dt>
<dd>Path to pipeline configuration YAML file</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>FileNotFoundError</code></dt>
<dd>If config file doesn't exist</dd>
<dt><code>ValueError</code></dt>
<dd>If configuration validation fails</dd>
<dt><code>RuntimeError</code></dt>
<dd>If model initialization fails</dd>
</dl>
<h2 id="note">Note</h2>
<ul>
<li>Configuration is automatically validated before use</li>
<li>Model loading may take several seconds on first run</li>
<li>GPU will be used if available for both detection and recognition</li>
<li>Logs detailed initialization information</li>
</ul></div>
<h3>Methods</h3>
<dl>
<dt id="src.pipeline.alpr_pipeline.ALPRPipeline.get_statistics"><code class="name flex">
<span>def <span class="ident">get_statistics</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_statistics(self) -&gt; dict:
    &#34;&#34;&#34;
    Get current pipeline statistics.

    Returns:
        dict: Statistics including:
            - frame_count: Total frames processed
            - track_count: Current active track count
            - recognized_count: Tracks with recognized text
            - avg_track_age: Average track age
            - avg_ocr_confidence: Average OCR confidence

    Example:
        &gt;&gt;&gt; stats = pipeline.get_statistics()
        &gt;&gt;&gt; print(f&#34;Processed {stats[&#39;frame_count&#39;]} frames, &#34;
        ...       f&#34;{stats[&#39;recognized_count&#39;]} plates recognized&#34;)
    &#34;&#34;&#34;
    summary = get_track_summary(self.tracks)

    return {
        &#34;frame_count&#34;: self.frame_count,
        &#34;track_count&#34;: summary[&#34;total&#34;],
        &#34;active_count&#34;: summary[&#34;active&#34;],
        &#34;recognized_count&#34;: summary[&#34;recognized&#34;],
        &#34;avg_track_age&#34;: summary[&#34;avg_age&#34;],
        &#34;avg_ocr_confidence&#34;: summary[&#34;avg_ocr_confidence&#34;],
    }</code></pre>
</details>
<div class="desc"><p>Get current pipeline statistics.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Statistics including:
- frame_count: Total frames processed
- track_count: Current active track count
- recognized_count: Tracks with recognized text
- avg_track_age: Average track age
- avg_ocr_confidence: Average OCR confidence</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; stats = pipeline.get_statistics()
&gt;&gt;&gt; print(f&quot;Processed {stats['frame_count']} frames, &quot;
...       f&quot;{stats['recognized_count']} plates recognized&quot;)
</code></pre></div>
</dd>
<dt id="src.pipeline.alpr_pipeline.ALPRPipeline.process_frame"><code class="name flex">
<span>def <span class="ident">process_frame</span></span>(<span>self, frame: numpy.ndarray) ‑> Dict[int, <a title="src.tracking.tracker.PlateTrack" href="../tracking/tracker.html#src.tracking.tracker.PlateTrack">PlateTrack</a>]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_frame(self, frame: np.ndarray) -&gt; Dict[int, PlateTrack]:
    &#34;&#34;&#34;
    Process a single video frame through the complete ALPR pipeline.

    Executes the 4-stage pipeline:
    1. Detection + Tracking: Run YOLOv8 with ByteTrack
    2. Track Management: Update track states
    3. Conditional OCR: Run OCR only when needed
    4. Cleanup: Remove lost tracks

    Args:
        frame: Input frame in BGR format (OpenCV convention)
              Shape: (height, width, 3)

    Returns:
        Dict[int, PlateTrack]: Dictionary of active tracks indexed by track ID.
                              Each PlateTrack contains current state including
                              bbox, text, confidence, and age.

    Raises:
        ValueError: If frame is invalid
        RuntimeError: If processing fails

    Example:
        &gt;&gt;&gt; tracks = pipeline.process_frame(frame)
        &gt;&gt;&gt; print(f&#34;Active tracks: {len(tracks)}&#34;)
        &gt;&gt;&gt; for track_id, track in tracks.items():
        ...     print(f&#34;  {track_id}: {track.text or &#39;No text&#39;} &#34;
        ...           f&#34;(age={track.age}, conf={track.ocr_confidence:.2f})&#34;)

    Note:
        - Track IDs persist across frames (ByteTrack maintains identity)
        - OCR is only run when should_run_ocr() returns True
        - Lost tracks are kept for max_age frames before removal
    &#34;&#34;&#34;
    # Validate input
    if not isinstance(frame, np.ndarray):
        raise ValueError(f&#34;Frame must be numpy array, got {type(frame)}&#34;)
    if frame.ndim != 3 or frame.shape[2] != 3:
        raise ValueError(f&#34;Frame must be BGR image with shape (H, W, 3), got {frame.shape}&#34;)

    logger.debug(f&#34;Processing frame {self.frame_count} (shape={frame.shape})&#34;)

    # ===== Stage 1: Detection + Tracking =====
    logger.debug(&#34;Stage 1: Running detection + tracking&#34;)

    try:
        # Run YOLOv8 tracking (ByteTrack integration)
        results = self.detection_model.track(
            source=frame,
            conf=self.config[&#34;detection&#34;][&#34;confidence_threshold&#34;],
            iou=self.config[&#34;detection&#34;][&#34;iou_threshold&#34;],
            tracker=&#34;bytetrack.yaml&#34;,
            persist=True,  # Crucial: maintains track IDs across frames
            verbose=False,
        )
    except Exception as e:
        logger.error(f&#34;Detection + tracking failed: {e}&#34;)
        raise RuntimeError(f&#34;Detection stage failed: {e}&#34;)

    # ===== Stage 2: Update Track States =====
    logger.debug(&#34;Stage 2: Updating track states&#34;)

    current_track_ids = set()
    ocr_count = 0

    # Parse detection results
    for result in results:
        # Check if any detections exist
        if not hasattr(result, &#34;boxes&#34;) or result.boxes is None:
            logger.debug(&#34;No detections in this frame&#34;)
            continue

        boxes = result.boxes

        # Check if tracks exist
        if boxes.id is None:
            logger.debug(&#34;No tracks assigned by ByteTrack&#34;)
            continue

        # Process each tracked detection
        for box in boxes:
            # Extract track information
            track_id = int(box.id.item())
            bbox = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]
            confidence = float(box.conf.item())

            # Convert bbox to tuple of integers
            bbox_tuple = tuple(map(int, bbox))

            # Create new track or update existing
            if track_id not in self.tracks:
                self.tracks[track_id] = PlateTrack(
                    track_id=track_id, bbox=bbox_tuple, confidence=confidence
                )
                logger.info(
                    f&#34;New track detected: ID={track_id}, bbox={bbox_tuple}, conf={
                        confidence:.3f}&#34;
                )
            else:
                self.tracks[track_id].update(bbox=bbox_tuple, confidence=confidence)
                logger.debug(f&#34;Track updated: ID={track_id}, age={self.tracks[track_id].age}&#34;)

            current_track_ids.add(track_id)

            # ===== Stage 3: Conditional OCR =====
            track = self.tracks[track_id]

            # Check if OCR should be run for this track
            if track.should_run_ocr(self.config[&#34;tracking&#34;]):
                logger.debug(
                    f&#34;OCR triggered for track {track_id} &#34;
                    f&#34;(age={
                        track.age}, frames_since_last_ocr={
                        track.frames_since_last_ocr})&#34;
                )

                try:
                    # Crop plate from frame
                    x1, y1, x2, y2 = bbox_tuple
                    cropped = frame[y1:y2, x1:x2]

                    if cropped.size == 0:
                        logger.warning(f&#34;Empty crop for track {track_id}, skipping OCR&#34;)
                        continue

                    # Use adaptive preprocessing (handled internally by recognize_text)
                    preprocessing_config = self.config.get(&#34;preprocessing&#34;, {})
                    use_adaptive_preprocessing = preprocessing_config.get(
                        &#34;enable_enhancement&#34;, False
                    )

                    # Run OCR with adaptive preprocessing enabled
                    text, ocr_conf = recognize_text(
                        preprocessed_image=cropped,
                        ocr_model=self.ocr_model,
                        config=self.config[&#34;recognition&#34;],
                        enable_adaptive_preprocessing=use_adaptive_preprocessing,
                        preprocessing_config=preprocessing_config,
                    )

                    # Update track with OCR results
                    track.update_text(text=text, confidence=ocr_conf)
                    ocr_count += 1

                    if text:
                        logger.info(
                            f&#39;Track {track_id} recognized: &#34;{text}&#34; (confidence={
                                ocr_conf:.3f})&#39;
                        )
                    else:
                        logger.debug(f&#34;Track {track_id}: No valid text recognized&#34;)

                except Exception as e:
                    logger.error(f&#34;OCR failed for track {track_id}: {e}&#34;)
                    # Continue processing other tracks
                    continue

    # ===== Stage 4: Cleanup Lost Tracks =====
    logger.debug(&#34;Stage 4: Cleaning up lost tracks&#34;)

    # Mark tracks that disappeared as lost
    lost_track_ids = set(self.tracks.keys()) - current_track_ids

    for track_id in lost_track_ids:
        self.tracks[track_id].mark_lost()
        logger.debug(f&#34;Track {track_id} marked as lost&#34;)

    # Remove inactive tracks older than max_age
    max_age = self.config[&#34;tracking&#34;].get(&#34;max_age&#34;, 30)
    initial_count = len(self.tracks)

    self.tracks = {
        tid: track
        for tid, track in self.tracks.items()
        if track.is_active or track.age &lt; max_age
    }

    removed_count = initial_count - len(self.tracks)
    if removed_count &gt; 0:
        logger.info(f&#34;Removed {removed_count} old lost tracks&#34;)

    # Increment frame counter
    self.frame_count += 1

    # Log summary
    summary = get_track_summary(self.tracks)
    logger.info(
        f&#39;Frame {self.frame_count}: {summary[&#34;active&#34;]} active tracks, &#39;
        f&#39;{summary[&#34;recognized&#34;]} recognized, {ocr_count} OCR runs&#39;
    )

    return self.tracks</code></pre>
</details>
<div class="desc"><p>Process a single video frame through the complete ALPR pipeline.</p>
<p>Executes the 4-stage pipeline:
1. Detection + Tracking: Run YOLOv8 with ByteTrack
2. Track Management: Update track states
3. Conditional OCR: Run OCR only when needed
4. Cleanup: Remove lost tracks</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>frame</code></strong></dt>
<dd>Input frame in BGR format (OpenCV convention)
Shape: (height, width, 3)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[int, PlateTrack]</code></dt>
<dd>Dictionary of active tracks indexed by track ID.
Each PlateTrack contains current state including
bbox, text, confidence, and age.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If frame is invalid</dd>
<dt><code>RuntimeError</code></dt>
<dd>If processing fails</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; tracks = pipeline.process_frame(frame)
&gt;&gt;&gt; print(f&quot;Active tracks: {len(tracks)}&quot;)
&gt;&gt;&gt; for track_id, track in tracks.items():
...     print(f&quot;  {track_id}: {track.text or 'No text'} &quot;
...           f&quot;(age={track.age}, conf={track.ocr_confidence:.2f})&quot;)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Track IDs persist across frames (ByteTrack maintains identity)</li>
<li>OCR is only run when should_run_ocr() returns True</li>
<li>Lost tracks are kept for max_age frames before removal</li>
</ul></div>
</dd>
<dt id="src.pipeline.alpr_pipeline.ALPRPipeline.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; None:
    &#34;&#34;&#34;
    Reset pipeline state (clear all tracks and counters).

    Useful when starting to process a new video or after processing is complete.

    Example:
        &gt;&gt;&gt; pipeline.reset()
        &gt;&gt;&gt; print(f&#34;Tracks cleared: {len(pipeline.tracks)}&#34;)
        Tracks cleared: 0
    &#34;&#34;&#34;
    logger.info(&#34;Resetting pipeline state&#34;)
    self.tracks.clear()
    self.frame_count = 0
    logger.info(&#34;Pipeline reset complete&#34;)</code></pre>
</details>
<div class="desc"><p>Reset pipeline state (clear all tracks and counters).</p>
<p>Useful when starting to process a new video or after processing is complete.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; pipeline.reset()
&gt;&gt;&gt; print(f&quot;Tracks cleared: {len(pipeline.tracks)}&quot;)
Tracks cleared: 0
</code></pre></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.pipeline" href="index.html">src.pipeline</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.pipeline.alpr_pipeline.ALPRPipeline" href="#src.pipeline.alpr_pipeline.ALPRPipeline">ALPRPipeline</a></code></h4>
<ul class="">
<li><code><a title="src.pipeline.alpr_pipeline.ALPRPipeline.get_statistics" href="#src.pipeline.alpr_pipeline.ALPRPipeline.get_statistics">get_statistics</a></code></li>
<li><code><a title="src.pipeline.alpr_pipeline.ALPRPipeline.process_frame" href="#src.pipeline.alpr_pipeline.ALPRPipeline.process_frame">process_frame</a></code></li>
<li><code><a title="src.pipeline.alpr_pipeline.ALPRPipeline.reset" href="#src.pipeline.alpr_pipeline.ALPRPipeline.reset">reset</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
