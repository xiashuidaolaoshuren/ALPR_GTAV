<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.detection API documentation</title>
<meta name="description" content="License Plate Detection Module …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.detection</code></h1>
</header>
<section id="section-intro">
<p>License Plate Detection Module</p>
<p>Uses YOLOv8 for detecting license plates in images/video frames.
Provides model loading, inference, and visualization capabilities.</p>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="src.detection.config" href="config.html">src.detection.config</a></code></dt>
<dd>
<div class="desc"><p>Detection Module Configuration …</p></div>
</dd>
<dt><code class="name"><a title="src.detection.model" href="model.html">src.detection.model</a></code></dt>
<dd>
<div class="desc"><p>Detection Model Module …</p></div>
</dd>
<dt><code class="name"><a title="src.detection.utils" href="utils.html">src.detection.utils</a></code></dt>
<dd>
<div class="desc"><p>Detection Utility Functions …</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.detection.batch_detect_plates"><code class="name flex">
<span>def <span class="ident">batch_detect_plates</span></span>(<span>frames: List[numpy.ndarray],<br>model,<br>conf_threshold: float = 0.25,<br>iou_threshold: float = 0.45) ‑> List[List[Tuple[int, int, int, int, float]]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def batch_detect_plates(
    frames: List[np.ndarray], model, conf_threshold: float = 0.25, iou_threshold: float = 0.45
) -&gt; List[List[Tuple[int, int, int, int, float]]]:
    &#34;&#34;&#34;
    Detect license plates in multiple frames (batch processing).

    Args:
        frames: List of input images in BGR format.
        model: Loaded YOLO model instance.
        conf_threshold: Confidence threshold for detections.
        iou_threshold: IOU threshold for NMS.

    Returns:
        List of detection lists, one for each input frame.
        Each detection list has the same format as detect_plates().

    Raises:
        ValueError: If frames list is empty or contains invalid arrays.
        RuntimeError: If batch inference fails.

    Note:
        - Batch processing is more efficient than processing frames individually
        - All frames should have similar dimensions for optimal performance
        - May require significant GPU memory for large batches
    &#34;&#34;&#34;
    # Implementation for future optimization
    logger.warning(&#34;batch_detect_plates() not yet implemented - future task&#34;)</code></pre>
</details>
<div class="desc"><p>Detect license plates in multiple frames (batch processing).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>frames</code></strong></dt>
<dd>List of input images in BGR format.</dd>
<dt><strong><code>model</code></strong></dt>
<dd>Loaded YOLO model instance.</dd>
<dt><strong><code>conf_threshold</code></strong></dt>
<dd>Confidence threshold for detections.</dd>
<dt><strong><code>iou_threshold</code></strong></dt>
<dd>IOU threshold for NMS.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>List of detection lists, one for each input frame.
Each detection list has the same format as detect_plates().</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If frames list is empty or contains invalid arrays.</dd>
<dt><code>RuntimeError</code></dt>
<dd>If batch inference fails.</dd>
</dl>
<h2 id="note">Note</h2>
<ul>
<li>Batch processing is more efficient than processing frames individually</li>
<li>All frames should have similar dimensions for optimal performance</li>
<li>May require significant GPU memory for large batches</li>
</ul></div>
</dd>
<dt id="src.detection.compute_iou"><code class="name flex">
<span>def <span class="ident">compute_iou</span></span>(<span>box1: Tuple[int, int, int, int], box2: Tuple[int, int, int, int]) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_iou(box1: Tuple[int, int, int, int], box2: Tuple[int, int, int, int]) -&gt; float:
    &#34;&#34;&#34;
    Compute Intersection over Union (IoU) for two bounding boxes.

    Args:
        box1: First bounding box in format (x1, y1, x2, y2).
             (x1, y1) is top-left corner, (x2, y2) is bottom-right corner.
        box2: Second bounding box in same format as box1.

    Returns:
        IoU value between 0.0 and 1.0:
        - 0.0: No overlap between boxes
        - 1.0: Boxes are identical
        - Values in between: Degree of overlap

    Raises:
        ValueError: If box coordinates are invalid (e.g., x1 &gt; x2 or y1 &gt; y2).

    Example:
        &gt;&gt;&gt; box1 = (100, 100, 200, 200)  # 100x100 box
        &gt;&gt;&gt; box2 = (150, 150, 250, 250)  # 100x100 box, partially overlapping
        &gt;&gt;&gt; iou = compute_iou(box1, box2)
        &gt;&gt;&gt; print(f&#34;IoU: {iou:.3f}&#34;)
        IoU: 0.143

    Note:
        - Used for tracking, NMS (Non-Maximum Suppression), and evaluation
        - Common thresholds: 0.5 for detection matching, 0.3-0.5 for tracking
        - Efficient implementation using vectorized operations
    &#34;&#34;&#34;
    # Validate box coordinates
    if box1[0] &gt; box1[2] or box1[1] &gt; box1[3]:
        raise ValueError(f&#34;Invalid box1 coordinates: {box1}&#34;)
    if box2[0] &gt; box2[2] or box2[1] &gt; box2[3]:
        raise ValueError(f&#34;Invalid box2 coordinates: {box2}&#34;)

    # Compute intersection coordinates
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])

    # Compute intersection area
    inter_width = max(0, x2_inter - x1_inter)
    inter_height = max(0, y2_inter - y1_inter)
    intersection_area = inter_width * inter_height

    # Compute union area
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - intersection_area

    # Avoid division by zero
    if union_area == 0:
        return 0.0

    # Compute IoU
    iou = intersection_area / union_area
    return float(iou)</code></pre>
</details>
<div class="desc"><p>Compute Intersection over Union (IoU) for two bounding boxes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>box1</code></strong></dt>
<dd>First bounding box in format (x1, y1, x2, y2).
(x1, y1) is top-left corner, (x2, y2) is bottom-right corner.</dd>
<dt><strong><code>box2</code></strong></dt>
<dd>Second bounding box in same format as box1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>IoU value between 0.0 and 1.0:
- 0.0: No overlap between boxes
- 1.0: Boxes are identical
- Values in between: Degree of overlap</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If box coordinates are invalid (e.g., x1 &gt; x2 or y1 &gt; y2).</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; box1 = (100, 100, 200, 200)  # 100x100 box
&gt;&gt;&gt; box2 = (150, 150, 250, 250)  # 100x100 box, partially overlapping
&gt;&gt;&gt; iou = compute_iou(box1, box2)
&gt;&gt;&gt; print(f&quot;IoU: {iou:.3f}&quot;)
IoU: 0.143
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Used for tracking, NMS (Non-Maximum Suppression), and evaluation</li>
<li>Common thresholds: 0.5 for detection matching, 0.3-0.5 for tracking</li>
<li>Efficient implementation using vectorized operations</li>
</ul></div>
</dd>
<dt id="src.detection.crop_detections"><code class="name flex">
<span>def <span class="ident">crop_detections</span></span>(<span>frame: numpy.ndarray,<br>detections: List[Tuple[int, int, int, int, float]],<br>padding: int = 0) ‑> List[numpy.ndarray]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crop_detections(
    frame: np.ndarray, detections: List[Tuple[int, int, int, int, float]], padding: int = 0
) -&gt; List[np.ndarray]:
    &#34;&#34;&#34;
    Crop detected regions from frame.

    Args:
        frame: Input image in BGR format.
        detections: List of detections in format [(x1, y1, x2, y2, confidence), ...].
        padding: Additional padding (in pixels) to add around each crop.
                Useful for including context around the detection.

    Returns:
        List of cropped image regions (numpy arrays), one for each detection.
        Returns empty list if no detections.

    Raises:
        ValueError: If frame is invalid or detections are out of bounds.

    Example:
        &gt;&gt;&gt; import cv2
        &gt;&gt;&gt; frame = cv2.imread(&#39;image.jpg&#39;)
        &gt;&gt;&gt; detections = [(100, 100, 200, 150, 0.95)]
        &gt;&gt;&gt; crops = crop_detections(frame, detections, padding=5)
        &gt;&gt;&gt; for i, crop in enumerate(crops):
        ...     cv2.imwrite(f&#39;crop_{i}.jpg&#39;, crop)

    Note:
        - Crops are automatically clipped to image boundaries
        - Padding is useful for OCR preprocessing (context helps recognition)
        - Cropped regions maintain the original image format (BGR)
    &#34;&#34;&#34;
    crops = []
    height, width = frame.shape[:2]

    for x1, y1, x2, y2, conf in detections:
        # Add padding and clip to image boundaries
        x1_pad = max(0, x1 - padding)
        y1_pad = max(0, y1 - padding)
        x2_pad = min(width, x2 + padding)
        y2_pad = min(height, y2 + padding)

        # Crop region
        crop = frame[y1_pad:y2_pad, x1_pad:x2_pad]
        crops.append(crop)

    logger.debug(f&#34;Cropped {len(crops)} detections from frame&#34;)
    return crops</code></pre>
</details>
<div class="desc"><p>Crop detected regions from frame.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>frame</code></strong></dt>
<dd>Input image in BGR format.</dd>
<dt><strong><code>detections</code></strong></dt>
<dd>List of detections in format [(x1, y1, x2, y2, confidence), &hellip;].</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Additional padding (in pixels) to add around each crop.
Useful for including context around the detection.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>List of cropped image regions (numpy arrays), one for each detection.
Returns empty list if no detections.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If frame is invalid or detections are out of bounds.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import cv2
&gt;&gt;&gt; frame = cv2.imread('image.jpg')
&gt;&gt;&gt; detections = [(100, 100, 200, 150, 0.95)]
&gt;&gt;&gt; crops = crop_detections(frame, detections, padding=5)
&gt;&gt;&gt; for i, crop in enumerate(crops):
...     cv2.imwrite(f'crop_{i}.jpg', crop)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Crops are automatically clipped to image boundaries</li>
<li>Padding is useful for OCR preprocessing (context helps recognition)</li>
<li>Cropped regions maintain the original image format (BGR)</li>
</ul></div>
</dd>
<dt id="src.detection.detect_plates"><code class="name flex">
<span>def <span class="ident">detect_plates</span></span>(<span>frame: numpy.ndarray,<br>model,<br>conf_threshold: float = 0.25,<br>iou_threshold: float = 0.45,<br>min_area: int = 1000) ‑> List[Tuple[int, int, int, int, float]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detect_plates(
    frame: np.ndarray,
    model,
    conf_threshold: float = 0.25,
    iou_threshold: float = 0.45,
    min_area: int = 1000,
) -&gt; List[Tuple[int, int, int, int, float]]:
    &#34;&#34;&#34;
    Detect license plates in a single frame.

    Args:
        frame: Input image in BGR format (OpenCV convention). Should be a
              numpy array with shape (height, width, 3).
        model: Loaded YOLO model instance from load_detection_model().
        conf_threshold: Confidence threshold for detections (0.0 to 1.0).
                       Detections with confidence below this value are filtered out.
                       Lower values detect more plates but increase false positives.
        iou_threshold: IOU (Intersection over Union) threshold for Non-Maximum
                      Suppression (0.0 to 1.0). Controls how much overlap is allowed
                      between detections. Higher values allow more overlapping boxes.
        min_area: Minimum bounding box area (pixels) to filter tiny false positives.
                 Default 1000px filters boxes smaller than ~32×32. Set to 0 to disable.

    Returns:
        List of detections, where each detection is a tuple:
        (x1, y1, x2, y2, confidence)
        - x1, y1: Top-left corner coordinates (integers)
        - x2, y2: Bottom-right corner coordinates (integers)
        - confidence: Detection confidence score (float, 0.0 to 1.0)

        Returns empty list if no plates are detected.

    Raises:
        ValueError: If frame is not a valid numpy array or has incorrect shape.
        RuntimeError: If inference fails due to model or input issues.

    Example:
        &gt;&gt;&gt; import cv2
        &gt;&gt;&gt; frame = cv2.imread(&#39;test_image.jpg&#39;)
        &gt;&gt;&gt; detections = detect_plates(frame, model, conf_threshold=0.5)
        &gt;&gt;&gt; print(f&#34;Detected {len(detections)} license plates&#34;)
        &gt;&gt;&gt; for x1, y1, x2, y2, conf in detections:
        ...     print(f&#34;Plate at ({x1},{y1})-({x2},{y2}) with confidence {conf:.2f}&#34;)

    Note:
        - Input frame should not be preprocessed (model handles resizing internally)
        - Coordinates are in the original image space (not normalized)
        - For batch processing, call this function for each frame
    &#34;&#34;&#34;
    # Validate input frame
    if not isinstance(frame, np.ndarray):
        raise ValueError(f&#34;Frame must be a numpy array, got {type(frame)}&#34;)

    if frame.ndim != 3 or frame.shape[2] != 3:
        raise ValueError(f&#34;Frame must have shape (height, width, 3), got {frame.shape}&#34;)

    logger.debug(f&#34;Running detection on frame of shape {frame.shape}&#34;)

    try:
        # Run YOLOv8 inference
        results = model.predict(frame, conf=conf_threshold, iou=iou_threshold, verbose=False)

        detections = []

        # Parse results
        for result in results:
            # Check if boxes exist
            if not hasattr(result, &#34;boxes&#34;) or result.boxes is None:
                logger.debug(&#34;No boxes found in results&#34;)
                continue

            boxes = result.boxes

            # Extract each detection
            for box in boxes:
                # Get coordinates in xyxy format (x1, y1, x2, y2)
                if hasattr(box, &#34;xyxy&#34;) and len(box.xyxy) &gt; 0:
                    coords = box.xyxy[0].cpu().numpy()
                    x1, y1, x2, y2 = coords

                    # Get confidence score
                    confidence = float(box.conf[0].cpu().numpy())

                    # Convert to integers and append
                    detections.append((int(x1), int(y1), int(x2), int(y2), confidence))

        logger.info(f&#34;Detected {len(detections)} plates with conf &gt;= {conf_threshold:.2f}&#34;)

        # Filter by minimum area to remove tiny false positives
        if min_area &gt; 0:
            original_count = len(detections)
            detections = [
                det for det in detections if (det[2] - det[0]) * (det[3] - det[1]) &gt;= min_area
            ]
            if len(detections) &lt; original_count:
                logger.info(
                    f&#34;Filtered {original_count - len(detections)} small detections &#34;
                    f&#34;(area &lt; {min_area}px)&#34;
                )

        # Apply additional NMS to remove near-duplicate detections
        # (more aggressive than YOLOv8&#39;s internal NMS)
        if len(detections) &gt; 1:
            detections = apply_nms(detections, iou_threshold=0.3)

        # Log detection details in debug mode
        if detections:
            logger.debug(&#34;Detection details:&#34;)
            for i, (x1, y1, x2, y2, conf) in enumerate(detections, 1):
                w, h = x2 - x1, y2 - y1
                logger.debug(f&#34;  {i}. ({x1},{y1})-({x2},{y2}) size={w}x{h} conf={conf:.3f}&#34;)

        return detections

    except Exception as e:
        logger.error(f&#34;Detection failed: {type(e).__name__}: {str(e)}&#34;)
        raise RuntimeError(
            f&#34;Detection inference failed: {str(e)}\n&#34;
            &#34;Please check that the model and input are compatible.&#34;
        )</code></pre>
</details>
<div class="desc"><p>Detect license plates in a single frame.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>frame</code></strong></dt>
<dd>Input image in BGR format (OpenCV convention). Should be a
numpy array with shape (height, width, 3).</dd>
<dt><strong><code>model</code></strong></dt>
<dd>Loaded YOLO model instance from load_detection_model().</dd>
<dt><strong><code>conf_threshold</code></strong></dt>
<dd>Confidence threshold for detections (0.0 to 1.0).
Detections with confidence below this value are filtered out.
Lower values detect more plates but increase false positives.</dd>
<dt><strong><code>iou_threshold</code></strong></dt>
<dd>IOU (Intersection over Union) threshold for Non-Maximum
Suppression (0.0 to 1.0). Controls how much overlap is allowed
between detections. Higher values allow more overlapping boxes.</dd>
<dt><strong><code>min_area</code></strong></dt>
<dd>Minimum bounding box area (pixels) to filter tiny false positives.
Default 1000px filters boxes smaller than ~32×32. Set to 0 to disable.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>List of detections, where each detection is a tuple:
(x1, y1, x2, y2, confidence)
- x1, y1: Top-left corner coordinates (integers)
- x2, y2: Bottom-right corner coordinates (integers)
- confidence: Detection confidence score (float, 0.0 to 1.0)</p>
<p>Returns empty list if no plates are detected.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If frame is not a valid numpy array or has incorrect shape.</dd>
<dt><code>RuntimeError</code></dt>
<dd>If inference fails due to model or input issues.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import cv2
&gt;&gt;&gt; frame = cv2.imread('test_image.jpg')
&gt;&gt;&gt; detections = detect_plates(frame, model, conf_threshold=0.5)
&gt;&gt;&gt; print(f&quot;Detected {len(detections)} license plates&quot;)
&gt;&gt;&gt; for x1, y1, x2, y2, conf in detections:
...     print(f&quot;Plate at ({x1},{y1})-({x2},{y2}) with confidence {conf:.2f}&quot;)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Input frame should not be preprocessed (model handles resizing internally)</li>
<li>Coordinates are in the original image space (not normalized)</li>
<li>For batch processing, call this function for each frame</li>
</ul></div>
</dd>
<dt id="src.detection.draw_bounding_boxes"><code class="name flex">
<span>def <span class="ident">draw_bounding_boxes</span></span>(<span>frame: numpy.ndarray,<br>detections: List[Tuple[int, int, int, int, float]],<br>color: Tuple[int, int, int] = (0, 255, 0),<br>thickness: int = 2,<br>show_confidence: bool = True,<br>font_scale: float = 0.5,<br>text_labels: List[str] | None = None) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def draw_bounding_boxes(
    frame: np.ndarray,
    detections: List[Tuple[int, int, int, int, float]],
    color: Tuple[int, int, int] = (0, 255, 0),
    thickness: int = 2,
    show_confidence: bool = True,
    font_scale: float = 0.5,
    text_labels: Optional[List[str]] = None,
) -&gt; np.ndarray:
    &#34;&#34;&#34;
    Draw bounding boxes on frame with optional confidence and text labels.

    Args:
        frame: Input image in BGR format (numpy array).
        detections: List of detections in format [(x1, y1, x2, y2, confidence), ...].
                   Coordinates should be integers in pixel space.
        color: Box color in BGR format. Default is green (0, 255, 0).
        thickness: Line thickness in pixels. Default is 2.
        show_confidence: If True, display confidence score above each box.
        font_scale: Font size for confidence labels. Default is 0.5.
        text_labels: Optional list of text labels (e.g., recognized plate text) to display
                    below each bounding box. Length must match number of detections.
                    Added in Task 19 for pipeline integration.

    Returns:
        Annotated frame with bounding boxes drawn. Returns a copy of the input frame.

    Raises:
        ValueError: If frame is not a valid numpy array, detections format is incorrect,
                   or text_labels length doesn&#39;t match detections length.

    Example:
        &gt;&gt;&gt; import cv2
        &gt;&gt;&gt; frame = cv2.imread(&#39;image.jpg&#39;)
        &gt;&gt;&gt; detections = [(100, 100, 200, 150, 0.95), (300, 200, 400, 250, 0.87)]
        &gt;&gt;&gt; # With text labels for recognized plates
        &gt;&gt;&gt; text_labels = [&#39;ABC123&#39;, &#39;XYZ789&#39;]
        &gt;&gt;&gt; annotated = draw_bounding_boxes(frame, detections, text_labels=text_labels)
        &gt;&gt;&gt; cv2.imwrite(&#39;annotated.jpg&#39;, annotated)

    Note:
        - This function does not modify the input frame (creates a copy)
        - For real-time visualization, consider reducing thickness for better performance
        - Coordinates outside image boundaries are clipped automatically by cv2
        - text_labels parameter added for Task 19 pipeline integration
    &#34;&#34;&#34;
    # Validate input frame
    if not isinstance(frame, np.ndarray):
        raise ValueError(f&#34;Frame must be a numpy array, got {type(frame)}&#34;)

    if frame.ndim != 3:
        raise ValueError(f&#34;Frame must have 3 dimensions, got {frame.ndim}&#34;)

    # Validate text_labels if provided
    if text_labels is not None and len(text_labels) != len(detections):
        raise ValueError(
            f&#34;text_labels length ({len(text_labels)}) must match &#34;
            f&#34;detections length ({len(detections)})&#34;
        )

    # Create a copy to avoid modifying the original
    annotated = frame.copy()

    # Draw each detection
    for i, detection in enumerate(detections):
        if len(detection) != 5:
            logger.warning(
                f&#34;Invalid detection format (expected 5 elements, got {
                    len(detection)}): {detection}&#34;
            )
            continue

        x1, y1, x2, y2, conf = detection

        # Ensure coordinates are integers
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)

        # Draw rectangle
        cv2.rectangle(annotated, (x1, y1), (x2, y2), color, thickness)

        # Add confidence label if requested
        if show_confidence:
            # Format confidence with 2 decimal places
            label = f&#34;{conf:.2f}&#34;

            # Calculate label size for background
            label_size, baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness
            )

            # Position label above the box (or below if box is at top of image)
            label_y = y1 - 10 if y1 &gt; 20 else y2 + 20

            # Draw background rectangle for better readability
            cv2.rectangle(
                annotated,
                (x1, label_y - label_size[1] - baseline),
                (x1 + label_size[0], label_y + baseline),
                color,
                -1,  # Filled rectangle
            )

            # Draw text
            cv2.putText(
                annotated,
                label,
                (x1, label_y),
                cv2.FONT_HERSHEY_SIMPLEX,
                font_scale,
                (0, 0, 0),  # Black text for contrast
                thickness=1,
                lineType=cv2.LINE_AA,
            )

        # Add text label below box if provided (Task 19 integration)
        if text_labels is not None and text_labels[i]:
            text = text_labels[i]

            # Calculate text size for background
            text_size, baseline = cv2.getTextSize(
                text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness
            )

            # Position text below the box (with some padding)
            text_y = y2 + text_size[1] + 15

            # Ensure text stays within frame bounds
            if text_y + baseline &gt; frame.shape[0]:
                text_y = y1 - 15  # Draw above box if no space below

            # Draw background rectangle for better readability
            cv2.rectangle(
                annotated,
                (x1, text_y - text_size[1] - baseline),
                (x1 + text_size[0], text_y + baseline),
                (0, 0, 0),  # Black background
                -1,  # Filled rectangle
            )

            # Draw text
            cv2.putText(
                annotated,
                text,
                (x1, text_y),
                cv2.FONT_HERSHEY_SIMPLEX,
                font_scale,
                (255, 255, 255),  # White text for contrast
                thickness=1,
                lineType=cv2.LINE_AA,
            )

    logger.debug(f&#34;Drew {len(detections)} bounding boxes on frame&#34;)
    return annotated</code></pre>
</details>
<div class="desc"><p>Draw bounding boxes on frame with optional confidence and text labels.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>frame</code></strong></dt>
<dd>Input image in BGR format (numpy array).</dd>
<dt><strong><code>detections</code></strong></dt>
<dd>List of detections in format [(x1, y1, x2, y2, confidence), &hellip;].
Coordinates should be integers in pixel space.</dd>
<dt><strong><code>color</code></strong></dt>
<dd>Box color in BGR format. Default is green (0, 255, 0).</dd>
<dt><strong><code>thickness</code></strong></dt>
<dd>Line thickness in pixels. Default is 2.</dd>
<dt><strong><code>show_confidence</code></strong></dt>
<dd>If True, display confidence score above each box.</dd>
<dt><strong><code>font_scale</code></strong></dt>
<dd>Font size for confidence labels. Default is 0.5.</dd>
<dt><strong><code>text_labels</code></strong></dt>
<dd>Optional list of text labels (e.g., recognized plate text) to display
below each bounding box. Length must match number of detections.
Added in Task 19 for pipeline integration.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Annotated frame with bounding boxes drawn. Returns a copy of the input frame.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If frame is not a valid numpy array, detections format is incorrect,
or text_labels length doesn't match detections length.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import cv2
&gt;&gt;&gt; frame = cv2.imread('image.jpg')
&gt;&gt;&gt; detections = [(100, 100, 200, 150, 0.95), (300, 200, 400, 250, 0.87)]
&gt;&gt;&gt; # With text labels for recognized plates
&gt;&gt;&gt; text_labels = ['ABC123', 'XYZ789']
&gt;&gt;&gt; annotated = draw_bounding_boxes(frame, detections, text_labels=text_labels)
&gt;&gt;&gt; cv2.imwrite('annotated.jpg', annotated)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>This function does not modify the input frame (creates a copy)</li>
<li>For real-time visualization, consider reducing thickness for better performance</li>
<li>Coordinates outside image boundaries are clipped automatically by cv2</li>
<li>text_labels parameter added for Task 19 pipeline integration</li>
</ul></div>
</dd>
<dt id="src.detection.filter_detections_by_size"><code class="name flex">
<span>def <span class="ident">filter_detections_by_size</span></span>(<span>detections: List[Tuple[int, int, int, int, float]],<br>min_width: int = 20,<br>min_height: int = 10,<br>max_width: int | None = None,<br>max_height: int | None = None) ‑> List[Tuple[int, int, int, int, float]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_detections_by_size(
    detections: List[Tuple[int, int, int, int, float]],
    min_width: int = 20,
    min_height: int = 10,
    max_width: Optional[int] = None,
    max_height: Optional[int] = None,
) -&gt; List[Tuple[int, int, int, int, float]]:
    &#34;&#34;&#34;
    Filter detections based on bounding box size constraints.

    Args:
        detections: List of detections in format [(x1, y1, x2, y2, confidence), ...].
        min_width: Minimum width in pixels. Detections smaller than this are filtered out.
        min_height: Minimum height in pixels. Detections smaller than this are filtered out.
        max_width: Maximum width in pixels. If None, no upper limit.
        max_height: Maximum height in pixels. If None, no upper limit.

    Returns:
        Filtered list of detections that meet size constraints.

    Example:
        &gt;&gt;&gt; detections = [(10, 10, 15, 15, 0.9), (100, 100, 250, 180, 0.95)]
        &gt;&gt;&gt; filtered = filter_detections_by_size(detections, min_width=30, min_height=20)
        &gt;&gt;&gt; print(f&#34;Kept {len(filtered)} out of {len(detections)} detections&#34;)
        Kept 1 out of 2 detections

    Note:
        - Useful for filtering out tiny false positives or unrealistic detections
        - Typical license plates: width 100-400 pixels, height 30-150 pixels (depends on resolution)
        - Consider image resolution when setting thresholds
    &#34;&#34;&#34;
    filtered = []

    for x1, y1, x2, y2, conf in detections:
        width = x2 - x1
        height = y2 - y1

        # Check minimum size
        if width &lt; min_width or height &lt; min_height:
            continue

        # Check maximum size if specified
        if max_width is not None and width &gt; max_width:
            continue
        if max_height is not None and height &gt; max_height:
            continue

        filtered.append((x1, y1, x2, y2, conf))

    logger.debug(f&#34;Size filtering: {len(detections)} -&gt; {len(filtered)} detections&#34;)
    return filtered</code></pre>
</details>
<div class="desc"><p>Filter detections based on bounding box size constraints.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>detections</code></strong></dt>
<dd>List of detections in format [(x1, y1, x2, y2, confidence), &hellip;].</dd>
<dt><strong><code>min_width</code></strong></dt>
<dd>Minimum width in pixels. Detections smaller than this are filtered out.</dd>
<dt><strong><code>min_height</code></strong></dt>
<dd>Minimum height in pixels. Detections smaller than this are filtered out.</dd>
<dt><strong><code>max_width</code></strong></dt>
<dd>Maximum width in pixels. If None, no upper limit.</dd>
<dt><strong><code>max_height</code></strong></dt>
<dd>Maximum height in pixels. If None, no upper limit.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Filtered list of detections that meet size constraints.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; detections = [(10, 10, 15, 15, 0.9), (100, 100, 250, 180, 0.95)]
&gt;&gt;&gt; filtered = filter_detections_by_size(detections, min_width=30, min_height=20)
&gt;&gt;&gt; print(f&quot;Kept {len(filtered)} out of {len(detections)} detections&quot;)
Kept 1 out of 2 detections
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>Useful for filtering out tiny false positives or unrealistic detections</li>
<li>Typical license plates: width 100-400 pixels, height 30-150 pixels (depends on resolution)</li>
<li>Consider image resolution when setting thresholds</li>
</ul></div>
</dd>
<dt id="src.detection.load_detection_model"><code class="name flex">
<span>def <span class="ident">load_detection_model</span></span>(<span>model_path: str, device: str = 'auto')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_detection_model(model_path: str, device: str = &#34;auto&#34;):
    &#34;&#34;&#34;
    Load pre-trained YOLOv8 model for license plate detection.

    Args:
        model_path: Path to YOLOv8 .pt weights file. Should be a valid file path
                   pointing to a trained YOLOv8 model in PyTorch format.
        device: Device to load model on. Options:
               - &#39;cuda&#39;: Use GPU acceleration (requires CUDA)
               - &#39;cpu&#39;: Use CPU only
               - &#39;auto&#39;: Automatically select GPU if available, otherwise CPU

    Returns:
        Loaded YOLO model instance ready for inference.

    Raises:
        FileNotFoundError: If model_path does not exist.
        RuntimeError: If model fails to load due to compatibility issues
                     or corrupted weights.
        ImportError: If required dependencies (ultralytics) are not installed.

    Example:
        &gt;&gt;&gt; model = load_detection_model(&#39;models/detection/yolov8n.pt&#39;)
        &gt;&gt;&gt; print(f&#34;Model loaded on device: {model.device}&#34;)

    Note:
        - The model file can be large (10-100 MB) and should not be committed to Git
        - First-time usage may download additional dependencies
        - GPU acceleration significantly improves inference speed
    &#34;&#34;&#34;
    # Validate model path
    if not os.path.exists(model_path):
        logger.error(f&#34;Model file not found: {model_path}&#34;)
        raise FileNotFoundError(
            f&#34;Model file not found: {model_path}\n&#34;
            &#34;Please download the model using: python models/detection/download_model.py&#34;
        )

    # Determine device
    if device == &#34;auto&#34;:
        device = &#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;
        logger.info(f&#34;Auto-selected device: {device}&#34;)

    logger.info(f&#34;Loading YOLOv8 model from: {model_path}&#34;)
    logger.info(f&#34;Target device: {device}&#34;)

    try:
        # Load YOLO model
        model = YOLO(model_path)

        # YOLOv8 handles device placement internally, but we can specify it
        if hasattr(model, &#34;to&#34;):
            model.to(device)

        # Log model information
        logger.info(&#34;✓ Model loaded successfully&#34;)
        logger.info(f&#34;  Model type: {type(model).__name__}&#34;)

        # Get model details if available
        if hasattr(model, &#34;names&#34;):
            logger.info(f&#34;  Classes: {len(model.names) if model.names else &#39;N/A&#39;}&#34;)

        return model

    except Exception as e:
        logger.error(f&#34;Failed to load model: {type(e).__name__}: {str(e)}&#34;)
        raise RuntimeError(
            f&#34;Model loading failed: {str(e)}\n&#34;
            &#34;The model file may be corrupted or incompatible. &#34;
            &#34;Try re-downloading it using: python models/detection/download_model.py&#34;
        )</code></pre>
</details>
<div class="desc"><p>Load pre-trained YOLOv8 model for license plate detection.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_path</code></strong></dt>
<dd>Path to YOLOv8 .pt weights file. Should be a valid file path
pointing to a trained YOLOv8 model in PyTorch format.</dd>
<dt><strong><code>device</code></strong></dt>
<dd>Device to load model on. Options:
- 'cuda': Use GPU acceleration (requires CUDA)
- 'cpu': Use CPU only
- 'auto': Automatically select GPU if available, otherwise CPU</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Loaded YOLO model instance ready for inference.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>FileNotFoundError</code></dt>
<dd>If model_path does not exist.</dd>
<dt><code>RuntimeError</code></dt>
<dd>If model fails to load due to compatibility issues
or corrupted weights.</dd>
<dt><code>ImportError</code></dt>
<dd>If required dependencies (ultralytics) are not installed.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; model = load_detection_model('models/detection/yolov8n.pt')
&gt;&gt;&gt; print(f&quot;Model loaded on device: {model.device}&quot;)
</code></pre>
<h2 id="note">Note</h2>
<ul>
<li>The model file can be large (10-100 MB) and should not be committed to Git</li>
<li>First-time usage may download additional dependencies</li>
<li>GPU acceleration significantly improves inference speed</li>
</ul></div>
</dd>
<dt id="src.detection.validate_model"><code class="name flex">
<span>def <span class="ident">validate_model</span></span>(<span>model) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_model(model) -&gt; bool:
    &#34;&#34;&#34;
    Validate that the loaded model can perform inference.

    Args:
        model: Loaded YOLO model instance to validate.

    Returns:
        True if model validation succeeds, False otherwise.

    Raises:
        RuntimeError: If validation fails critically.

    Note:
        - Creates a dummy input (e.g., 640x640 random image) for testing
        - Useful for debugging model loading issues
        - Should be called after load_detection_model() in production
    &#34;&#34;&#34;
    logger.info(&#34;Validating model with dummy input...&#34;)

    try:
        # Create dummy input image (640x640 RGB)
        dummy_input = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)

        # Run inference
        results = model.predict(dummy_input, verbose=False)

        # Check if results are valid
        if results is None:
            logger.error(&#34;Model returned None results&#34;)
            return False

        logger.info(&#34;✓ Model validation successful&#34;)
        logger.info(f&#34;  Inference test passed with dummy {dummy_input.shape} input&#34;)

        return True

    except Exception as e:
        logger.error(f&#34;Model validation failed: {type(e).__name__}: {str(e)}&#34;)
        raise RuntimeError(
            f&#34;Model validation failed: {str(e)}\n&#34;
            &#34;The model may not be compatible with the current environment.&#34;
        )</code></pre>
</details>
<div class="desc"><p>Validate that the loaded model can perform inference.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>Loaded YOLO model instance to validate.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>True if model validation succeeds, False otherwise.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>RuntimeError</code></dt>
<dd>If validation fails critically.</dd>
</dl>
<h2 id="note">Note</h2>
<ul>
<li>Creates a dummy input (e.g., 640x640 random image) for testing</li>
<li>Useful for debugging model loading issues</li>
<li>Should be called after load_detection_model() in production</li>
</ul></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.detection.DetectionConfig"><code class="flex name class">
<span>class <span class="ident">DetectionConfig</span></span>
<span>(</span><span>config_dict: Dict[str, Any])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DetectionConfig:
    &#34;&#34;&#34;Configuration class for the detection module.&#34;&#34;&#34;

    def __init__(self, config_dict: Dict[str, Any]):
        &#34;&#34;&#34;
        Initialize detection configuration from dictionary.

        Args:
            config_dict: Dictionary containing detection configuration parameters.
                        Expected to have &#39;detection&#39; key with nested parameters.

        Raises:
            KeyError: If required configuration keys are missing.
            ValueError: If configuration values are invalid.
        &#34;&#34;&#34;
        if &#34;detection&#34; not in config_dict:
            raise KeyError(&#34;Missing &#39;detection&#39; key in configuration&#34;)

        detection_config = config_dict[&#34;detection&#34;]

        # Load configuration parameters
        self.model_path = detection_config.get(&#34;model_path&#34;, &#34;models/detection/yolov8n.pt&#34;)
        self.confidence_threshold = detection_config.get(&#34;confidence_threshold&#34;, 0.25)
        self.iou_threshold = detection_config.get(&#34;iou_threshold&#34;, 0.45)
        self.img_size = detection_config.get(&#34;img_size&#34;, 640)
        self.device = detection_config.get(&#34;device&#34;, &#34;cuda&#34;)
        self.max_det = detection_config.get(&#34;max_det&#34;, 100)

        logger.info(
            f&#34;Detection config loaded: model_path={self.model_path}, &#34;
            f&#34;conf={self.confidence_threshold}, iou={self.iou_threshold}&#34;
        )

    def validate(self) -&gt; bool:
        &#34;&#34;&#34;
        Validate configuration parameters.

        Returns:
            True if configuration is valid.

        Raises:
            FileNotFoundError: If model_path does not exist.
            ValueError: If threshold values are out of valid range.
        &#34;&#34;&#34;
        # Validate model path existence
        if not os.path.exists(self.model_path):
            logger.error(f&#34;Model file not found: {self.model_path}&#34;)
            raise FileNotFoundError(
                f&#34;Model file not found: {self.model_path}. &#34;
                &#34;Please download the model or update the path in configs/pipeline_config.yaml&#34;
            )

        # Validate confidence threshold
        if not 0.0 &lt;= self.confidence_threshold &lt;= 1.0:
            raise ValueError(
                &#34;Confidence threshold must be between 0.0 and 1.0, &#34;
                f&#34;got {self.confidence_threshold}&#34;
            )

        # Validate IOU threshold
        if not 0.0 &lt;= self.iou_threshold &lt;= 1.0:
            raise ValueError(
                &#34;IOU threshold must be between 0.0 and 1.0, &#34; f&#34;got {self.iou_threshold}&#34;
            )

        # Validate image size
        if self.img_size &lt;= 0:
            raise ValueError(f&#34;Image size must be positive, got {self.img_size}&#34;)

        # Validate device
        if self.device not in [&#34;cuda&#34;, &#34;cpu&#34;, &#34;auto&#34;]:
            logger.warning(
                f&#34;Device &#39;{self.device}&#39; not in [&#39;cuda&#39;, &#39;cpu&#39;, &#39;auto&#39;]. &#34;
                &#34;Will attempt to use it anyway.&#34;
            )

        logger.info(&#34;Detection configuration validated successfully&#34;)
        return True

    def to_dict(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;
        Convert configuration to dictionary.

        Returns:
            Dictionary containing all configuration parameters.
        &#34;&#34;&#34;
        return {
            &#34;model_path&#34;: self.model_path,
            &#34;confidence_threshold&#34;: self.confidence_threshold,
            &#34;iou_threshold&#34;: self.iou_threshold,
            &#34;img_size&#34;: self.img_size,
            &#34;device&#34;: self.device,
            &#34;max_det&#34;: self.max_det,
        }

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;String representation of configuration.&#34;&#34;&#34;
        return (
            f&#34;DetectionConfig(model_path=&#39;{self.model_path}&#39;, &#34;
            f&#34;confidence_threshold={self.confidence_threshold}, &#34;
            f&#34;iou_threshold={self.iou_threshold}, &#34;
            f&#34;img_size={self.img_size}, &#34;
            f&#34;device=&#39;{self.device}&#39;, &#34;
            f&#34;max_det={self.max_det})&#34;
        )</code></pre>
</details>
<div class="desc"><p>Configuration class for the detection module.</p>
<p>Initialize detection configuration from dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config_dict</code></strong></dt>
<dd>Dictionary containing detection configuration parameters.
Expected to have 'detection' key with nested parameters.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>KeyError</code></dt>
<dd>If required configuration keys are missing.</dd>
<dt><code>ValueError</code></dt>
<dd>If configuration values are invalid.</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="src.detection.DetectionConfig.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;
    Convert configuration to dictionary.

    Returns:
        Dictionary containing all configuration parameters.
    &#34;&#34;&#34;
    return {
        &#34;model_path&#34;: self.model_path,
        &#34;confidence_threshold&#34;: self.confidence_threshold,
        &#34;iou_threshold&#34;: self.iou_threshold,
        &#34;img_size&#34;: self.img_size,
        &#34;device&#34;: self.device,
        &#34;max_det&#34;: self.max_det,
    }</code></pre>
</details>
<div class="desc"><p>Convert configuration to dictionary.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary containing all configuration parameters.</p></div>
</dd>
<dt id="src.detection.DetectionConfig.validate"><code class="name flex">
<span>def <span class="ident">validate</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate(self) -&gt; bool:
    &#34;&#34;&#34;
    Validate configuration parameters.

    Returns:
        True if configuration is valid.

    Raises:
        FileNotFoundError: If model_path does not exist.
        ValueError: If threshold values are out of valid range.
    &#34;&#34;&#34;
    # Validate model path existence
    if not os.path.exists(self.model_path):
        logger.error(f&#34;Model file not found: {self.model_path}&#34;)
        raise FileNotFoundError(
            f&#34;Model file not found: {self.model_path}. &#34;
            &#34;Please download the model or update the path in configs/pipeline_config.yaml&#34;
        )

    # Validate confidence threshold
    if not 0.0 &lt;= self.confidence_threshold &lt;= 1.0:
        raise ValueError(
            &#34;Confidence threshold must be between 0.0 and 1.0, &#34;
            f&#34;got {self.confidence_threshold}&#34;
        )

    # Validate IOU threshold
    if not 0.0 &lt;= self.iou_threshold &lt;= 1.0:
        raise ValueError(
            &#34;IOU threshold must be between 0.0 and 1.0, &#34; f&#34;got {self.iou_threshold}&#34;
        )

    # Validate image size
    if self.img_size &lt;= 0:
        raise ValueError(f&#34;Image size must be positive, got {self.img_size}&#34;)

    # Validate device
    if self.device not in [&#34;cuda&#34;, &#34;cpu&#34;, &#34;auto&#34;]:
        logger.warning(
            f&#34;Device &#39;{self.device}&#39; not in [&#39;cuda&#39;, &#39;cpu&#39;, &#39;auto&#39;]. &#34;
            &#34;Will attempt to use it anyway.&#34;
        )

    logger.info(&#34;Detection configuration validated successfully&#34;)
    return True</code></pre>
</details>
<div class="desc"><p>Validate configuration parameters.</p>
<h2 id="returns">Returns</h2>
<p>True if configuration is valid.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>FileNotFoundError</code></dt>
<dd>If model_path does not exist.</dd>
<dt><code>ValueError</code></dt>
<dd>If threshold values are out of valid range.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="../index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="src.detection.config" href="config.html">src.detection.config</a></code></li>
<li><code><a title="src.detection.model" href="model.html">src.detection.model</a></code></li>
<li><code><a title="src.detection.utils" href="utils.html">src.detection.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.detection.batch_detect_plates" href="#src.detection.batch_detect_plates">batch_detect_plates</a></code></li>
<li><code><a title="src.detection.compute_iou" href="#src.detection.compute_iou">compute_iou</a></code></li>
<li><code><a title="src.detection.crop_detections" href="#src.detection.crop_detections">crop_detections</a></code></li>
<li><code><a title="src.detection.detect_plates" href="#src.detection.detect_plates">detect_plates</a></code></li>
<li><code><a title="src.detection.draw_bounding_boxes" href="#src.detection.draw_bounding_boxes">draw_bounding_boxes</a></code></li>
<li><code><a title="src.detection.filter_detections_by_size" href="#src.detection.filter_detections_by_size">filter_detections_by_size</a></code></li>
<li><code><a title="src.detection.load_detection_model" href="#src.detection.load_detection_model">load_detection_model</a></code></li>
<li><code><a title="src.detection.validate_model" href="#src.detection.validate_model">validate_model</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.detection.DetectionConfig" href="#src.detection.DetectionConfig">DetectionConfig</a></code></h4>
<ul class="">
<li><code><a title="src.detection.DetectionConfig.to_dict" href="#src.detection.DetectionConfig.to_dict">to_dict</a></code></li>
<li><code><a title="src.detection.DetectionConfig.validate" href="#src.detection.DetectionConfig.validate">validate</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
